# ğŸš€ Scalable Web Scraping Pipeline

## ğŸ“Œ Overview

This project is a **modular and scalable web scraping pipeline** designed to extract **verified email addresses** from company websites listed on the Europages directory.  
It was developed as part of a challenge to showcase the ability to build a flexible, industry-specific lead generation system using intelligent scraping techniques.

---

## ğŸ”„ Workflow

The pipeline focuses on a selected **sector** (e.g., `wines`) and filters by **supplier type** (e.g., `producers`). The scraping process includes the following steps:

### ğŸ§© 1. Directory Scraping
- Navigate to the Europages search results.
- Extract company names and their detail page links.

### ğŸŒ 2. Website Extraction
- Visit each companyâ€™s profile page.
- Retrieve the official website URL if available.

### ğŸ“§ 3. Email Extraction
- Visit each companyâ€™s website.
- Extract email addresses from the homepage or relevant subpages (e.g., `Contact`, `About Us`).

### ğŸ§¹ 4. Data Cleaning & Storage
- Clean and validate extracted data.
- Store the results in well-structured CSV files.
- Skip companies with missing or invalid websites to avoid `None` values.

---

## âš ï¸ Challenges Faced

### ğŸ”— Website Extraction
- Some companies do not provide website links.
- Others link to broken or invalid websites.
- These entries were skipped to ensure data integrity.

### ğŸ“­ Email Extraction
- Some websites do not list email addresses at all â€” verified manually.
- Pop-ups such as cookie consent or age verification gates often block access.
- Many pop-ups were in local languages (e.g., `"accepto"` instead of `"accept"`), making automation harder.
- Email addresses are often hidden in subpages (`Contact`, `About Us`), and their naming varies across languages.
- Handling these systematically would require more sophisticated logic or language models (LLMs).

---

## ğŸ§  Design Considerations

- âœ… **Modular Design**: Logic split across reusable, clear methods.
- ğŸ” **Scalable Architecture**: Easily extensible for new sectors or directories.
- âš™ï¸ **Configuration File** (`config.py`): Centralizes sector names, filters, and constants.
- ğŸ§ª **Debugging-Friendly**: Step-by-step logs and manageable function boundaries.

---

## ğŸ—ƒï¸ Output Format

Two CSV outputs per sector:

- `links_<sector>.csv` â†’ Contains company names and their website URLs.
- `emails_<sector>.csv` â†’ Contains company names, countries, and extracted emails.

> Companies without valid **websites** are excluded from the final CSVs.

---

## ğŸ§© What Could Be Improved

### ğŸš… 1. Performance Optimization
- The current scraping logic is sequential.
- Could be enhanced using:
  - Asynchronous I/O (`aiohttp`, `asyncio`)
  - Multithreading or multiprocessing
- Especially helpful when scaling to scrape thousands of companies.

### ğŸ§  2. Smarter Popup Handling
- Some websites display pop-ups in various languages or formats.
- A language-aware LLM or a custom popup detector could:
  - Automatically detect and close cookie consent, age verification, or modal overlays.

### ğŸ” 3. Robust Subpage Detection
- Emails are often located on subpages like `"Contact Us"`, `"Ãœber Uns"`, `"Contatti"`, etc.
- Current approach uses keyword matching.
- A semantic component recognition system or LLM-based classifier could improve multilingual accuracy.

---
